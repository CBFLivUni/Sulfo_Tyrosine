parts <- strsplit(x, "-", fixed=TRUE)[[1]]
paste(parts[-1], collapse="-")
})
temp_data <- data.frame(peptidoform_id = peptidoform_id,
Experiment_Tag = experiment_tags,
dataset_id = dataset_ids,
stringsAsFactors=FALSE)
BOI_expanded_data <- rbind(BOI_expanded_data, temp_data)
}
# Every Experiment Tag should have a match in metadata$Experiment.Tag. get the associated metadata$Instrument.Name for each
# preallocate column
BOI_expanded_data$Instrument_Name <- NA
for(i in 1:nrow(BOI_expanded_data)) {
experiment_tag <- BOI_expanded_data$Experiment_Tag[i]
matching_row <- metadata_df[metadata_df$Experiment.Tag == experiment_tag,]
if(nrow(matching_row) > 0) {
BOI_expanded_data$Instrument_Name[i] <- matching_row$Instrument.Name[1]
} else {
# match the dataset id to that of the manually annotated metadata
temp_dataset_id <- BOI_expanded_data$dataset_id[i]
manual_instrument <- manual_annotations_df[manual_annotations_df$dataset_id == temp_dataset_id, "Instrument_Name"] %>% unique()
# and assign the corresponding instrument
BOI_expanded_data$Instrument_Name[i] <- manual_instrument
}
}
return(BOI_expanded_data)
}
#### counting n times an instrument has contirbutedto the data in a bin
project_dir = "C:/Users/jtzve/Desktop/Sufo_Tyrosine/08_instrument_enrichment//"
data_dir <- "C:/Users/jtzve/Desktop/Sufo_Tyrosine/08_instrument_enrichment/in/"
gc()
########## METADATA #################
# all metadata from protein atlas
metadata <- read.csv(file = paste0(data_dir, "human_phosphobuild_metadata.csv"),
header = TRUE,
)
# instrument metadata - manually collected
instrument_metadata_all <- read.csv(file = paste0(data_dir, "instrument_metadata.csv"),
header = TRUE,
)
# we only need the first 4 columns where I have split the instruments into groups
instrument_metadata <- instrument_metadata[,1:4]
# we only need the first 4 columns where I have split the instruments into groups
instrument_metadata <- instrument_metadata_all[,1:4]
########## METADATA #################
# all metadata from protein atlas
metadata <- read.csv(file = paste0(data_dir, "human_phosphobuild_metadata.csv"),
header = TRUE,
)
# instrument metadata - manually collected
instrument_metadata_all <- read.csv(file = paste0(data_dir, "instrument_metadata.csv"),
header = TRUE,
)
# we only need the first 4 columns where I have split the instruments into groups
instrument_metadata <- instrument_metadata_all[,1:4]
metadata_for_mismatches <- read.csv(file = paste0(data_dir, "manually_annotated_exp_tags.csv"),
header = TRUE,
)
##################### BIN DATA #####################
# bin of interest
BOI <- read.csv(file = paste0(data_dir, "tyrosine_containing_foreground_inSwissProt.tsv"),
header = TRUE,
sep = "\t"
)
# one bin left
DECOY_minus1 <- read.csv(file = paste0(data_dir, "tyrosine_containing_DECOY_minus1_inSwissProt.tsv"),
header = TRUE,
sep = "\t"
)
DECOY_minus2 <- read.csv(file = paste0(data_dir, "tyrosine_containing_DECOY_minus2_inSwissProt.tsv"),
header = TRUE,
sep = "\t"
) ## not read in as no unique entries not already in decoy minus 1
# mirror bin to BOI
DECOY1 <- read.csv(file = paste0(data_dir, "tyrosine_containing_DECOY1_inSwissProt.tsv"),
header = TRUE,
sep = "\t"
)
DECOY2 <- read.csv(file = paste0(data_dir, "tyrosine_containing_DECOY2_inSwissProt.tsv"),
header = TRUE,
sep = "\t"
)
DECOY3 <- read.csv(file = paste0(data_dir, "tyrosine_containing_DECOY3_inSwissProt.tsv"),
header = TRUE,
sep = "\t"
)
all_bins_background <- read.csv(file = paste0(data_dir, "tyrosine_containing_background_inSwissProt.tsv"),
header = TRUE,
sep = "\t"
)
# # we need the data with bins in rows and counts in columns.
# # we need counts split into a few different categories - instrument name, one column for each,
# # and then based on factors - sensitivity (documentation), and sensitivity (chatGPT)
# # then we run a bunch of chi squared tests
# apply the function to each dataset
BOI_instruments <- get_instruments_by_peptidoform(df = BOI,
metadata_df = metadata,
manual_annotations_df = metadata_for_mismatches)
DM1_instruments <- get_instruments_by_peptidoform(df = DECOY_minus1,
metadata_df = metadata,
manual_annotations_df = metadata_for_mismatches)
DM2_instruments <- get_instruments_by_peptidoform(df = DECOY_minus2,
metadata_df = metadata,
manual_annotations_df = metadata_for_mismatches)
D1_instruments <- get_instruments_by_peptidoform(df = DECOY1,
metadata_df = metadata,
manual_annotations_df = metadata_for_mismatches)
D2_instruments <- get_instruments_by_peptidoform(df = DECOY2,
metadata_df = metadata,
manual_annotations_df = metadata_for_mismatches)
D3_instruments <- get_instruments_by_peptidoform(df = DECOY3,
metadata_df = metadata,
manual_annotations_df = metadata_for_mismatches)
all_data_instruments <- get_instruments_by_peptidoform(df = all_bins_background,
metadata_df = metadata,
manual_annotations_df = metadata_for_mismatches)
# create a table where every row will be a bin ID
contingency_table <- data.frame(bin_ID = c("DM2", "DM1", "BOI", "D1", "D2", "D3", "all_bins"),
stringsAsFactors=FALSE)
# generate a named list of data frames to match the bin ID order
instrument_data_list <- list(
DM2 = DM2_instruments,
DM1 = DM1_instruments,
BOI = BOI_instruments,
D1 = D1_instruments,
D2 = D2_instruments,
D3 = D3_instruments,
all_bins = all_data_instruments
)
# create a column in the cintingency table for every instrument name:
# instrument_metadata$Instrument_Name
for (instrument_name in instrument_metadata$Instrument_Name) {
contingency_table[[instrument_name]] <- 0
}
# for every bin, populate counts
for (bin_name in names(instrument_data_list)) {
# get all instrument data
instrument_data <- instrument_data_list[[bin_name]]
#for every instrument
for (instrument_name in instrument_metadata$Instrument_Name) {
# count the number of times an instrument name appears
instrument_count <- sum(instrument_data$Instrument_Name == instrument_name)
contingency_table[contingency_table$bin_ID == bin_name, instrument_name] <- instrument_count
}
}
# set row names to remove non numeric col
rownames(contingency_table) <- contingency_table$bin_ID
contingency_table <- contingency_table[,-1]
##  filtering - some of the instruments aren't represented enough to be meaningful
# remove columns where less than 50 counts are present in total across the bins
# excluding the data for all bins
contingency_counts_subset <- contingency_table[, colSums(contingency_table[1:6,]) > 50]
# excluded instruments:
excluded <- contingency_table[, colSums(contingency_table[1:6,]) <= 50]
# LTQ FT
#
# LTQ Orbitrap Discovery
#
# Orbitrap Exploris 480
# These are the oldest and newest instruments; they have contributed a small fraction of
# the data - present in one dataset each and having between 1 and 3 associated experiment tags.
excluded_metadata <- metadata[metadata$Instrument.Name %in% names(excluded), ]
# we have our observed counts fot a chi sq goodness of fit test, now we need
# to generate the vector p (p: a vector of probabilities)
# in our case these probabilities are the expected rations of the instruments (as a fraction of the total data points)
# we can get expected count ratios by dividing the observed number across all bins by the total number of data points across all bins
# get observed
all_bins_instrument_counts <- contingency_counts_subset[nrow(contingency_counts_subset),]
# total is just their sum
all_bins_total_counts <- sum(all_bins_instrument_counts) # here i have not included the ones that did not pass the > 50 counts filter; is this correct?
# calculate expected ratios
expected_count_proportions_by_instrument <- all_bins_instrument_counts/all_bins_total_counts
# i dont think i need this, but good to have just in case
total_countsL_by_bin <- rowSums(contingency_counts_subset)
# Remove the row for all bins data
contingency_counts_for_goodness_of_fit <- contingency_counts_subset[-(nrow(contingency_counts_subset)),]
# baloonplot
# 1. convert the data as a table
dt <- as.table(as.matrix(contingency_counts_for_goodness_of_fit[,]))
# Generate the balloonplot with rotated column labels
balloonplot(t(dt), main ="Counts by Instrument", xlab ="", ylab="",
label = FALSE, show.margins = FALSE,
colsrt=90) # Rotate column labels to 90 degrees
#mosaic plot_ v1
mosaicplot(dt, shade = TRUE, las=2, main = "Counts by Instrument")
# mosaic plot v2
mosaic(t(dt), shade = TRUE, las=2, main = "Counts by Instrument")
goodness_of_fit_results <- chisq.test(contingency_counts_for_goodness_of_fit,
p = expected_count_proportions_by_instrument[1,])
expected_counts <- round(goodness_of_fit_results$expected, 4)
goodness_of_fit_results$residuals
assumption_of_5 <- 100*sum(expected_counts >= 5)/sum(expected_counts >= 0)
residuals <- goodness_of_fit_results$residuals
names(residuals)
residuals
categories <- colnames(residuals)
categories
# Basic Residuals Plot
plot(categories, residuals, type='h', lwd=2, col='blue', main='Residuals Plot',
xlab='Category', ylab='Residuals')
# Convert residuals and categories into a data frame
residuals_df <- data.frame(Category = colnamesnames(residuals), Residuals = residuals)
# Convert residuals and categories into a data frame
residuals_df <- data.frame(Category = colnames(residuals), Residuals = residuals)
# Load ggplot2
library(ggplot2)
# Create a plot using ggplot
ggplot(residuals_df, aes(x = Category, y = Residuals)) +
geom_hline(yintercept = 0, color = "red", linetype = "dashed") +  # Reference line at y=0
geom_point() +  # Add points
geom_segment(aes(xend = Category, yend = 0), color = "blue") +  # Add vertical lines
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  # Rotate x labels for readability
labs(title = "Residuals Plot", x = "Category", y = "Residuals")
residuals_df
# Convert residuals and categories into a data frame
residuals_df <- data.frame(Category = rownames(residuals), Residuals = residuals)
# Load ggplot2
library(ggplot2)
# Create a plot using ggplot
ggplot(residuals_df, aes(x = Category, y = Residuals)) +
geom_hline(yintercept = 0, color = "red", linetype = "dashed") +  # Reference line at y=0
geom_point() +  # Add points
geom_segment(aes(xend = Category, yend = 0), color = "blue") +  # Add vertical lines
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  # Rotate x labels for readability
labs(title = "Residuals Plot", x = "Category", y = "Residuals")
residuals_df
plot(residuals)
barplot(residuals)
residuals <- goodness_of_fit_results$stdres
residuals
residuals_2 <-
barplot(residuals) goodness_of_fit_results$residuals
residuals_2 <- goodness_of_fit_results$residuals
residuals_2
# Assuming 'observed_df' is your original dataframe with bin_ID, and m/z boundaries
# Create a dataframe for plotting with standardized residuals
residuals_data <- data.frame(bin_ID = rownames(contingency_counts_for_goodness_of_fit), Residuals = standardized_residuals)
standardized_residuals <- goodness_of_fit_results$stdres
# Assuming 'observed_df' is your original dataframe with bin_ID, and m/z boundaries
# Create a dataframe for plotting with standardized residuals
residuals_data <- data.frame(bin_ID = rownames(contingency_counts_for_goodness_of_fit), Residuals = standardized_residuals)
plot_title <- paste0("Standardized Residuals from Chi-squared Test")
# Plotting the standardized residuals
p <- ggplot(residuals_data, aes(x = reorder(bin_ID, lower_boundary), y = Residuals)) +
geom_point(size = 3) +
geom_hline(yintercept = c(-2, 2), linetype = "dashed", color = "red") + # Thresholds for significance
theme_minimal() +
labs(title = plot_title,
x = "Bin (Ordered by Lower m/z Boundary)",
y = "Standardized Residual") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Display the plot
print(p)
# Plotting the standardized residuals
p <- ggplot(residuals_data, aes(x = bin_ID, y = Residuals)) +
geom_point(size = 3) +
geom_hline(yintercept = c(-2, 2), linetype = "dashed", color = "red") + # Thresholds for significance
theme_minimal() +
labs(title = plot_title,
x = "Bin (Ordered by Lower m/z Boundary)",
y = "Standardized Residual") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Display the plot
print(p)
residuals_data
standardized_residuals
standardised_residuals <- goodness_of_fit_results$stdres
pdf(file = "residuals.pdf", width = 2000, heiht = 2000)
pdf(file = "residuals.pdf", width = 2000, height = 2000)
for (i in 1:ncol(standardised_residuals)){
residuals_data <- data.frame(bin_ID = rownames(contingency_counts_for_goodness_of_fit), Residuals = standardized_residuals[,i])
plot_title <- paste0("Standardized Residuals from Chi-squared Test",i)
# Plotting the standardized residuals
p <- ggplot(residuals_data, aes(x = bin_ID, y = Residuals)) +
geom_point(size = 3) +
geom_hline(yintercept = c(-2, 2), linetype = "dashed", color = "red") + # Thresholds for significance
theme_minimal() +
labs(title = plot_title,
x = "Bin (Ordered by Lower m/z Boundary)",
y = "Standardized Residual") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Display the plot
print(p)
}
dev.off()
pdf(file = "residuals.pdf", width = 2000, height = 2000)
for (i in 1:ncol(standardised_residuals)){
residuals_data <- data.frame(bin_ID = rownames(contingency_counts_for_goodness_of_fit), Residuals = standardized_residuals[,i])
plot_title <- paste0("Standardized Residuals from Chi-squared Test",i)
# Plotting the standardized residuals
p <- ggplot(residuals_data, aes(x = bin_ID, y = Residuals)) +
geom_point(size = 3) +
geom_hline(yintercept = c(-2, 2), linetype = "dashed", color = "red") + # Thresholds for significance
theme_minimal() +
labs(title = plot_title,
x = "Bin (Ordered by Lower m/z Boundary)",
y = "Standardized Residual") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Display the plot
print(p)
}
dev.off()
pdf(file = "residuals.pdf", width = 200, height = 200)
for (i in 1:ncol(standardised_residuals)){
residuals_data <- data.frame(bin_ID = rownames(contingency_counts_for_goodness_of_fit), Residuals = standardized_residuals[,i])
plot_title <- paste0("Standardized Residuals from Chi-squared Test",i)
# Plotting the standardized residuals
p <- ggplot(residuals_data, aes(x = bin_ID, y = Residuals)) +
geom_point(size = 3) +
geom_hline(yintercept = c(-2, 2), linetype = "dashed", color = "red") + # Thresholds for significance
theme_minimal() +
labs(title = plot_title,
x = "Bin (Ordered by Lower m/z Boundary)",
y = "Standardized Residual") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Display the plot
print(p)
}
dev.off()
pdf(file = "residuals.pdf", width = 8, height = 6)
for (i in 1:ncol(standardised_residuals)){
residuals_data <- data.frame(bin_ID = rownames(contingency_counts_for_goodness_of_fit), Residuals = standardized_residuals[,i])
plot_title <- paste0("Standardized Residuals from Chi-squared Test",i)
# Plotting the standardized residuals
p <- ggplot(residuals_data, aes(x = bin_ID, y = Residuals)) +
geom_point(size = 3) +
geom_hline(yintercept = c(-2, 2), linetype = "dashed", color = "red") + # Thresholds for significance
theme_minimal() +
labs(title = plot_title,
x = "Bin (Ordered by Lower m/z Boundary)",
y = "Standardized Residual") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Display the plot
print(p)
}
dev.off()
assumption_of_5 <- 100*sum(expected_counts >= 5)/sum(expected_counts >= 0)
assumption_of_5
# lets look at the different categories we cna group the isntruments by.
# release year - newer should be more sensitive; here we work with all instruments - before they have been excluded
table(instrument_metadata_all$release_year)
# 2011 seems to be the mi-dpoint; I set it to 2010 before when i was collecting the data
# lets use 2011
release_y_factor <- ifelse(instrument_metadata_all$release_year <= 2011,
"Before 2011",
"After 2011")
instrument_metadata$release_y_factor <- as.factor(release_y_factor)
# we include all instruments. - we already hace the contingency counts
# creat an empty data frame
contingency_counts_by_factors_releasey <- data.frame(row.names = rownames(contingency_counts))
# create a table where every row will be a bin ID
contingency_table <- data.frame(bin_ID = c("DM2", "DM1", "BOI", "D1", "D2", "D3", "all_bins"),
stringsAsFactors=FALSE)
# generate a named list of data frames to match the bin ID order
instrument_data_list <- list(
DM2 = DM2_instruments,
DM1 = DM1_instruments,
BOI = BOI_instruments,
D1 = D1_instruments,
D2 = D2_instruments,
D3 = D3_instruments,
all_bins = all_data_instruments
)
# create a column in the cintingency table for every instrument name:
# instrument_metadata$Instrument_Name
for (instrument_name in instrument_metadata$Instrument_Name) {
contingency_table[[instrument_name]] <- 0
}
# for every bin, populate counts
for (bin_name in names(instrument_data_list)) {
# get all instrument data
instrument_data <- instrument_data_list[[bin_name]]
#for every instrument
for (instrument_name in instrument_metadata$Instrument_Name) {
# count the number of times an instrument name appears
instrument_count <- sum(instrument_data$Instrument_Name == instrument_name)
contingency_table[contingency_table$bin_ID == bin_name, instrument_name] <- instrument_count
}
}
# set row names to remove non numeric col
rownames(contingency_table) <- contingency_table$bin_ID
contingency_table <- contingency_table[,-1]
##  filtering - some of the instruments aren't represented enough to be meaningful
# remove columns where less than 50 counts are present in total across the bins
# excluding the data for all bins
contingency_counts_subset <- contingency_table[, colSums(contingency_table[1:6,]) > 50]
# excluded instruments:
excluded <- contingency_table[, colSums(contingency_table[1:6,]) <= 50]
# LTQ FT
#
# LTQ Orbitrap Discovery
#
# Orbitrap Exploris 480
# These are the oldest and newest instruments; they have contributed a small fraction of
# the data - present in one dataset each and having between 1 and 3 associated experiment tags.
excluded_metadata <- metadata[metadata$Instrument.Name %in% names(excluded), ]
# we have our observed counts fot a chi sq goodness of fit test, now we need
# to generate the vector p (p: a vector of probabilities)
# in our case these probabilities are the expected rations of the instruments (as a fraction of the total data points)
# we can get expected count ratios by dividing the observed number across all bins by the total number of data points across all bins
# get observed
all_bins_instrument_counts <- contingency_counts_subset[nrow(contingency_counts_subset),]
# total is just their sum
all_bins_total_counts <- sum(all_bins_instrument_counts) # here i have not included the ones that did not pass the > 50 counts filter; is this correct?
# calculate expected ratios
expected_count_proportions_by_instrument <- all_bins_instrument_counts/all_bins_total_counts
# i dont think i need this, but good to have just in case
total_countsL_by_bin <- rowSums(contingency_counts_subset)
# we include all instruments. - we already hace the contingency counts
# creat an empty data frame
contingency_counts_by_factors_releasey <- data.frame(row.names = rownames(contingency_counts))
pdf(file = "residuals.pdf", width = 8, height = 6)
for (i in 1:ncol(standardised_residuals)){
residuals_data <- data.frame(bin_ID = rownames(contingency_counts_for_goodness_of_fit), Residuals = standardized_residuals[,i])
column_name <- colnames(standardized_residuals)[i]
plot_title <- paste0("Standardized Residuals from Chi-squared Test: ", column_name)
# Plotting the standardized residuals
p <- ggplot(residuals_data, aes(x = bin_ID, y = Residuals)) +
geom_point(size = 3) +
geom_hline(yintercept = c(-2, 2), linetype = "dashed", color = "red") + # Thresholds for significance
theme_minimal() +
labs(title = plot_title,
x = "Bin (Ordered by Lower m/z Boundary)",
y = "Standardized Residual") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Display the plot
print(p)
}
dev.off()
# we include all instruments. - we already hace the contingency counts
# create an empty data frame
contingency_counts_by_factors_releasey <- data.frame(row.names = rownames(contingency_counts_for_goodness_of_fit))
contingency_counts_by_factors_releasey
# Step 1: Create a logical vector for instruments released before 2011 and after
instruments_before_2011 <- instrument_metadata$Instrument_Name[instrument_metadata$release_y_factor == "Before 2011"]
instruments_after_2011 <- instrument_metadata$Instrument_Name[instrument_metadata$release_y_factor != "Before 2011"]
contingency_counts_for_goodness_of_fit
contingency_counts_subset
# we include all instruments. - we already hace the contingency counts
# create an empty data frame
contingency_counts_by_factors_releasey <- data.frame(row.names = rownames(contingency_table))
contingency_counts_by_factors_releasey
# we include all instruments. - we already hace the contingency counts
# create an empty data frame
contingency_counts_by_factors_releasey <- data.frame(row.names = rownames(contingency_table))
# Step 1: Create a logical vector for instruments released before 2011 and after
instruments_before_2011 <- instrument_metadata$Instrument_Name[instrument_metadata$release_y_factor == "Before 2011"]
instruments_after_2011 <- instrument_metadata$Instrument_Name[instrument_metadata$release_y_factor != "Before 2011"]
# Step 2: Match these instruments with columns in 'contingency_counts'
columns_before_2011 <- colnames(contingency_table) %in% instruments_before_2011
columns_after_2011 <- colnames(contingency_table) %in% instruments_after_2011
# Step 3: Sum counts by row for the matched columns
contingency_counts_by_factors_releasey$before2011 <- rowSums(contingency_table[, columns_before_2011], na.rm = TRUE)
contingency_counts_by_factors_releasey$after2011 <- rowSums(contingency_table[, columns_after_2011], na.rm = TRUE)
contingency_counts_by_factors_releasey
expected_count_proportions_by_instrument_by_factors_releasey <- contingency_counts_by_factors_releasey[7,]/sum(contingency_counts_by_factors_releasey[7,])
goodness_of_fit_by_year <- chisq.test(contingency_counts_by_factors_releasey[-7,],
p = expected_count_proportions_by_instrument_by_factors_releasey[1,])
expected_by_year <- goodness_of_fit_by_year$expected
goodness_of_fit_by_year$residuals
table(instrument_metadata_all$SpecDocs_mass_accuracy_external_cal)
table(instrument_metadata_all$SpecDocs_mass_accuracy_internal_cal)
# maybe we can split to sub 1 ppm internal vs > 1 ppm; only 4 instruments > 1 ppm
instrument_metadata_all$Instrument_Name[instrument_metadata_all$SpecDocs_mass_accuracy_internal_cal > 1]
# Add factor for internal calibration sensitivity (sub 1 ppm vs > 1 ppm)
instrument_metadata_all$internal_cal_factor <- ifelse(instrument_metadata_all$SpecDocs_mass_accuracy_internal_cal > 1, "Above 1 ppm", "Sub 1 ppm")
instruments_int_cal_over1 <- instrument_metadata_all$Instrument_Name[instrument_metadata_all$internal_cal_factor == "Above 1 ppm"]
instruments_int_cal_under1 <- instrument_metadata_all$Instrument_Name[instrument_metadata_all$internal_cal_factor == "Sub 1 ppm"]
# Step 2: Sum counts by row for the matched columns
contingency_counts_by_factors$instruments_int_cal_over1 <- rowSums(contingency_table[, instruments_int_cal_over1], na.rm = TRUE)
goodness_of_fit_results
expected_counts
View(expected_count_proportions_by_instrument)
goodness_of_fit_results
goodness_of_fit_by_year
expected_by_year
# lets look at the different categories we cna group the isntruments by.
# release year - newer should be more sensitive; here we work with all instruments - before they have been excluded
table(instrument_metadata_all$release_year)
table(instrument_metadata_all$SpecDocs_mass_accuracy_external_cal)
table(instrument_metadata_all$SpecDocs_mass_accuracy_internal_cal)
# these calibrations are not specific to MS2 acqisitions; havent looked at the publications, but asked chatGPT for general numbers - NB: these may be wrong
table(instrument_metadata_all$ChatGPT_mass_acc_ext_MSMS)
# lets look at the different categories we cna group the isntruments by.
# release year - newer should be more sensitive; here we work with all instruments - before they have been excluded
table(instrument_metadata_all$release_year)
# lets look at the different categories we cna group the isntruments by.
# release year - newer should be more sensitive; here we work with all instruments - before they have been excluded
table(instrument_metadata_all$release_year)
# 2011 seems to be the mi-dpoint; I set it to 2010 before when i was collecting the data
# lets use 2011
release_y_factor <- ifelse(instrument_metadata_all$release_year <= 2011,
"Before 2011",
"After 2011")
instrument_metadata$release_y_factor <- as.factor(release_y_factor)
# we include all instruments. - we already hace the contingency counts
# create an empty data frame
contingency_counts_by_factors_releasey <- data.frame(row.names = rownames(contingency_table))
# Step 1: Create a logical vector for instruments released before 2011 and after
instruments_before_2011 <- instrument_metadata$Instrument_Name[instrument_metadata$release_y_factor == "Before 2011"]
instruments_after_2011 <- instrument_metadata$Instrument_Name[instrument_metadata$release_y_factor != "Before 2011"]
# Step 2: Match these instruments with columns in 'contingency_counts'
columns_before_2011 <- colnames(contingency_table) %in% instruments_before_2011
columns_after_2011 <- colnames(contingency_table) %in% instruments_after_2011
contingency_counts_by_factors_releasey$after2011 <- rowSums(contingency_table[, columns_after_2011], na.rm = TRUE)
expected_count_proportions_by_instrument_by_factors_releasey <- contingency_counts_by_factors_releasey[7,]/sum(contingency_counts_by_factors_releasey[7,])
goodness_of_fit_by_year <- chisq.test(contingency_counts_by_factors_releasey[-7,],
p = expected_count_proportions_by_instrument_by_factors_releasey[1,])
contingency_counts_by_factors_releasey
expected_count_proportions_by_instrument_by_factors_releasey
expected_count_proportions_by_instrument_by_factors_releasey
contingency_counts_by_factors_releasey
